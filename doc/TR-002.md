# TR-002: UDP トランスポートの性能特性と最適化

**Document ID**: TR-002
**Project**: grebe — Stage/Interface 実行フレームワーク
**Date**: 2026-02-11
**Status**: Final (v0.2.0 Phase 9–9.2 完了時点)

---

## 概要

本レポートは、grebe v0.2.0 で実装した UDP トランスポートの性能特性を報告する。Phase 9 で基本実装を完了した後、Phase 9.1 でボトルネック調査（BM-H ベンチマーク）、Phase 9.2 で scatter-gather I/O および sendmmsg/recvmmsg バッチ化による最適化を実施した。

UDP トランスポートは grebe-sg（信号生成）と grebe-viewer（可視化）を独立プロセスとして接続するための経路であり、ネットワーク越しの運用も視野に入れた設計である。本調査では WSL2 と Windows native の 2 環境でスループット上限・ボトルネック・最適化効果を定量的に評価した。

### 主要な成果

| 指標 | 目標 | 達成値 | 環境 |
|------|------|--------|------|
| 1ch × 1 GSPS via UDP | 達成可能性 | **1,000 MSPS (0% drops)** | Windows native, 16KB datagram |
| 1ch 最大スループット | — | **3,371 MSPS** | Windows native, 65KB datagram |
| 4ch × 100 MSPS/ch | 0% drops | **0% drops** | Windows native, 4KB datagram |
| 4ch 最大スループット | — | **3,401 MSPS (合計)** | Windows native, 65KB datagram |
| 100 MSPS 0% drops (WSL2) | burst=1 で 36% drops | **burst=8 で 0% drops** | WSL2, sendmmsg |
| WSL2 スループット改善 | — | **+11%** (108 vs 98 MSPS) | WSL2, sendmmsg burst=64 |

---

## 1. UDP トランスポートアーキテクチャ

### 1.1 設計目標と位置づけ

UDP トランスポートは RDD §2.2 の動作モードのうち「独立プロセス / ネットワーク ingress」を担う。Pipe トランスポートがプロセス親子関係を前提とするのに対し、UDP は任意のホスト間で動作する。

設計上の制約:

- 外部ライブラリに依存しない（POSIX ソケット / Winsock のみ）
- FrameHeaderV2 プロトコルを Pipe と共有し、上流/下流の Stage 契約を変更しない
- データグラムサイズは環境に応じて自動制限する

### 1.2 プロトコル構造

1 データグラム = 1 フレームとして送信する。

```
┌─────────────────────────────────────────┐
│ FrameHeaderV2 (64 bytes)                │
│   magic: 'GFH2'                         │
│   sequence: 単調増加                      │
│   producer_ts_ns: 生成時刻               │
│   channel_count: 1–8                    │
│   block_length_samples: per channel     │
│   payload_bytes: ch × samples × 2      │
│   sample_rate_hz, sg_drops_total, ...   │
├─────────────────────────────────────────┤
│ Payload (channel-major int16_t)         │
│   [ch0: block_length_samples × 2 bytes] │
│   [ch1: block_length_samples × 2 bytes] │
│   ...                                   │
└─────────────────────────────────────────┘
```

ペイロードの最大サイズはデータグラムサイズからヘッダ 64 bytes を除いた残りであり、チャネル数で等分される:

```
block_length_samples = (datagram_size - 64) / (channel_count × 2)
```

### 1.3 送信・受信パス

```
grebe-sg                                grebe-viewer
┌──────────────────┐                    ┌──────────────────────┐
│ DataGenerator     │                    │ UdpConsumer          │
│   ↓               │                    │   recvfrom /         │
│ RingBuffer (SPSC) │                    │   recvmmsg (batch)   │
│   ↓               │                    │   ↓                  │
│ Sender thread     │   UDP datagram     │ TransportSource      │
│   UdpProducer     │───────────────────▶│   ↓                  │
│   sendmsg /       │                    │ N × RingBuffer       │
│   sendmmsg (batch)│                    │   ↓                  │
└──────────────────┘                    │ DecimationThread     │
                                         │   ↓                  │
                                         │ Renderer             │
                                         └──────────────────────┘
```

Phase 9.2 で送信パスに scatter-gather I/O（ヘッダとペイロードの分離送信）、および Linux 環境で sendmmsg/recvmmsg によるバッチ化を導入した。

---

## 2. ボトルネック調査

### 2.1 ループバックスループット上限

レート制限なし（全速送信）での WSL2 ループバック性能を測定した。

| シナリオ | ch | block_size | フレーム/秒 | MSPS | MB/s | drop rate |
|---|---|---|---|---|---|---|
| 1ch max | 1 | 668 | 155,278 | 103.7 | 197.8 | 0.78% |
| 1ch 256 | 1 | 256 | 157,884 | 40.4 | 77.1 | 0.50% |
| 1ch 64 | 1 | 64 | 153,764 | 9.8 | 18.8 | 0% |
| 2ch max | 2 | 334 | 157,671 | 52.7 | 200.9 | ~0% |
| 4ch max | 4 | 167 | 153,947 | 25.7 | 196.1 | 0.02% |

フレームレート上限は block_size によらず **約 155K frames/sec** で一定であり、ボトルネックがペイロード処理ではなくシステムコールオーバーヘッドにあることを示している。

### 2.2 フレームサイズ vs スループット特性

| block_size | bytes/frame | frames/sec | MSPS (1ch) | 効率 |
|---|---|---|---|---|
| 64 | 192 | 153,764 | 9.8 | 14.7 MSPS/100Kfps |
| 256 | 576 | 157,884 | 40.4 | 25.6 MSPS/100Kfps |
| 668 | 1,400 | 155,278 | 103.7 | 66.8 MSPS/100Kfps |

フレームレートがペイロードサイズにほぼ依存しない一方、MSPS はフレームサイズに比例する。この特性は「1 データグラムにより多くのサンプルを詰めるほど高効率」であることを意味し、データグラムサイズ拡大の動機となった。

### 2.3 目標レート到達性

レート制限付き送信で UDP が要求レートに追従できるかを測定した。

| 目標レート | 実測 MSPS | frames/sec | drop rate | 判定 |
|---|---|---|---|---|
| 1 MSPS | 1.0 | 1,497 | 0% | PASS |
| 10 MSPS | 10.0 | 14,970 | 0% | PASS |
| 100 MSPS | 99.1 | 148,292 | 0% | PASS |
| 1 GSPS | 97.3 | 145,591 | 1.33% | 上限到達 |

WSL2 環境では 100 MSPS まで 0% drops で安定動作する。1 GSPS では実効 97 MSPS（要求の 9.7%）に留まり、WSL2 の MTU 制限（1,400 bytes/datagram）による構造的な上限に到達した。

### 2.4 ボトルネック要因分析

#### システムコールオーバーヘッド（支配的）

```
フレーム間隔 = 1 / 155,000 fps ≈ 6.4 μs
sendto + recvfrom の合計オーバーヘッド ≈ 6.4 μs/frame
```

UDP ループバックはカーネル内コピーであり、ネットワーク遅延はゼロである。ボトルネックは純粋にシステムコール呼び出しのコンテキストスイッチとバッファコピーに起因する。

#### WSL2 MTU 制限（構造的）

WSL2 ループバックは 1,472 bytes を超えるデータグラムをドロップする（GitHub #6082 既知問題）。安全マージンを含め 1,400 bytes に制限した結果、1ch で最大 block_size = 668 samples となり、MSPS の理論上限が制約される。

#### Pipe との比較

```
UDP 上限:     104 MSPS (1ch, WSL2, 1400B datagram)
Pipe 上限:    1,000+ MSPS (同一プロセス内)
理論限界差:   ~10x (Pipe vs UDP)
```

UDP は per-frame のシステムコールコストが Pipe より高いが、100 MSPS まで 0% drops であるため実用上は十分である。

---

## 3. クロスプラットフォーム性能差

### 3.1 WSL2 vs Windows native（同一データグラムサイズ）

Windows native (MSVC Release) で同一ベンチマークを実行し、WSL2 との差を測定した。

#### 全速送信

| シナリオ | WSL2 fps | Win fps | WSL2 MSPS | Win MSPS | WSL2 drop | Win drop | fps 倍率 |
|---|---|---|---|---|---|---|---|
| 1ch max (668) | 155,278 | 219,995 | 103.7 | 147.0 | 0.78% | 0% | 1.42x |
| 1ch 256 | 157,884 | 222,299 | 40.4 | 56.9 | 0.50% | 0% | 1.41x |
| 1ch 64 | 153,764 | 233,483 | 9.8 | 14.9 | 0% | 0% | 1.52x |
| 2ch max (334) | 157,671 | 229,345 | 52.7 | 76.6 | ~0% | 0% | 1.45x |
| 4ch max (167) | 153,947 | 189,712 | 25.7 | 31.7 | 0.02% | 0% | 1.23x |

#### 目標レート制限

| 目標レート | WSL2 MSPS | Win MSPS | WSL2 drop | Win drop |
|---|---|---|---|---|
| 1 MSPS | 1.0 | 1.0 | 0% | 0% |
| 10 MSPS | 10.0 | 10.0 | 0% | 0% |
| 100 MSPS | 99.1 | 100.0 | 0% | 0% |
| 1 GSPS | 97.3 | 115.6 | 1.33% | 0% |

Windows native は全シナリオで **0% drops** を達成し、WSL2 の Hyper-V 仮想化オーバーヘッドが約 30% のコストを占めることが判明した（syscall あたり 6.4 μs → 4.5 μs）。

### 3.2 Windows native データグラムサイズ拡大

Windows ループバックは 65,535 bytes のデータグラムを許容する。MTU 制限を解除してスループット特性を測定した。

#### 1ch 全速送信

| datagram (bytes) | block_size | frames/sec | MSPS | MB/s | drop | MSPS 倍率 (vs 1400) |
|---|---|---|---|---|---|---|
| 1,400 | 668 | 227,108 | 151.7 | 280 | 0% | 1.0x |
| 4,000 | 1,968 | 199,098 | 391.8 | 760 | 0% | 2.6x |
| 16,000 | 7,968 | 166,531 | 1,327 | 2,539 | 0% | 8.7x |
| 32,000 | 15,968 | 142,708 | 2,279 | 4,358 | 0% | 15.0x |
| 65,000 | 32,468 | 103,819 | 3,371 | 6,447 | 0% | 22.2x |

スループットはデータグラムサイズに概ね比例する。これはシステムコール回数が律速であり、per-frame のサンプル数増加が直接スループットに反映されるためである。フレームレートは低下（227K → 104K fps）するが、per-frame サンプル数の増加が支配的であり、全サイズで 0% drops を維持した。

#### 1 GSPS 到達性

| datagram (bytes) | block_size | 実効 MSPS | frames/sec | drop | 1 GSPS 到達 |
|---|---|---|---|---|---|
| 1,400 | 668 | 117.7 | 176,260 | 0% | 11.8% |
| 4,000 | 1,968 | 371.1 | 188,576 | 0% | 37.1% |
| 16,000 | 7,968 | 1,000.0 | 125,502 | 0% | **達成** |
| 32,000 | 15,968 | 1,000.0 | 62,625 | 0% | 達成（余裕あり） |
| 65,000 | 32,468 | 1,000.0 | 30,800 | 0% | 達成（大幅余裕） |

**datagram_size ≥ 16,000 bytes で 1 GSPS (1ch) に到達**した。65KB datagram では最大 3,371 MSPS のヘッドルームがあり、1 GSPS ターゲットに対して 3.4 倍の余裕がある。

### 3.3 マルチチャネル性能

4ch での UDP スループットをデータグラムサイズ別に測定した。

#### 4ch 全速送信

| datagram (bytes) | block_size/ch | frames/sec | MSPS/ch | 合計 MSPS | drop |
|---|---|---|---|---|---|
| 1,400 | 167 | 207,226 | 34.6 | 138 | 0% |
| 4,000 | 492 | 208,084 | 102.4 | 410 | 0% |
| 16,000 | 1,992 | 178,474 | 355.5 | 1,422 | 0% |
| 32,000 | 3,992 | 142,652 | 569.5 | 2,278 | 0% |
| 65,000 | 8,117 | 104,754 | 850.3 | 3,401 | 0.02% |

#### 1ch vs 4ch 合計スループット比較

| datagram (bytes) | 1ch MSPS | 4ch 合計 MSPS | 効率 (合計/1ch) |
|---|---|---|---|
| 1,400 | 152 | 138 | 91% |
| 4,000 | 392 | 410 | 105% |
| 16,000 | 1,327 | 1,422 | 107% |
| 32,000 | 2,279 | 2,278 | 100% |
| 65,000 | 3,371 | 3,401 | 101% |

合計スループットはチャネル数にほぼ依存せず **約 3,400 MSPS** (65KB datagram) で一定である。ボトルネックがシステムコール + memcpy であり、チャネル数ではなくペイロードサイズで決定されるためである。チャネルを増やしても合計転送量は変わらず、1ch あたりの帯域が等分される。

#### 4ch 実用上の推奨設定

| 目標 | 推奨 datagram_size | 必要フレームレート |
|---|---|---|
| 10 MSPS/ch | 1,400 B（デフォルト） | ~60K fps |
| 100 MSPS/ch | ≥ 4,000 B | ~200K fps |
| 250 MSPS/ch (1 GSPS 合計) | ≥ 16,000 B | ~126K fps |
| 500 MSPS/ch (2 GSPS 合計) | ≥ 32,000 B | ~125K fps |

4ch × 1 GSPS/ch（4 GSPS 合計）は、必要帯域 8 GB/s がシステムコールベースの UDP では非現実的であり、達成には共有メモリ + ゼロコピーが必要となる。

---

## 4. データパス最適化

Phase 9.1 で特定した 2 つのボトルネック（per-frame システムコール、中間バッファへの memcpy）に対し、Phase 9.2 で以下の最適化を実施した。

### 4.1 scatter-gather I/O

Phase 9.1 の送信パスでは、ヘッダとペイロードを連結バッファに `memcpy` した後に `sendto()` で送信していた。Phase 9.2 ではヘッダとペイロードを分離したまま `sendmsg()` + `iovec[2]`（Linux）/ `WSASendTo()` + `WSABUF[2]`（Windows）でカーネルに直接渡す scatter-gather 方式に変更し、送信側の memcpy を廃止した。

```
Phase 9.1 (memcpy + sendto):
  [header] ──memcpy──┐
  [payload] ─memcpy──┤
                      ↓
                [send_buf_]  ──sendto()──▶ kernel

Phase 9.2 (scatter-gather):
  [header]  ──iovec[0]──┐
  [payload] ──iovec[1]──┤
                         ↓
                    sendmsg() ──▶ kernel  (memcpy 廃止)
```

#### 効果

| 指標 | Phase 9.1 (memcpy+sendto) | Phase 9.2 (scatter-gather) | Delta |
|---|---|---|---|
| WSL2 1ch max MSPS | ~104 MSPS | ~98 MSPS | −6%（ノイズ範囲） |
| Windows 65KB MSPS | 3,371 MSPS | 3,321 MSPS | −1.5%（ノイズ範囲） |

scatter-gather 単体では WSL2 環境で有意な改善は観測されなかった。memcpy 削減分の効果は WSL2 ネットワークスタックのオーバーヘッドに埋もれている。Windows native でも統計的に同等であり、**回帰なし**を確認した。

scatter-gather の価値は即時のスループット向上ではなく、大データグラム時の不要コピー排除による CPU 負荷低減と、sendmmsg バッチ化の前提条件としてのアーキテクチャ整備にある。

### 4.2 sendmmsg/recvmmsg バッチ化

Linux 環境で `sendmmsg()` / `recvmmsg()` による複数データグラムの一括送受信を実装した。

**送信側**: `send_frame()` が内部バッチに蓄積し、`burst_size` に達するか `flush()` 呼び出しで `sendmmsg()` 一括送信。

**受信側**: 内部キューが空のとき `recvmmsg()` + `MSG_WAITFORONE` で最大 N データグラムを一括受信し、キューから 1 フレームずつ返却。

Windows では sendmmsg/recvmmsg に相当する API がないため、scatter-gather `WSASendTo` のみ（バッチなし）にフォールバックする。

#### 100 MSPS ターゲットでの改善

最も顕著な改善は、100 MSPS ターゲットでのドロップ率である。

| burst_size | 実効 MSPS | drop rate |
|---|---|---|
| 1（scatter-gather のみ） | 63.8 | **36.20%** |
| 8（sendmmsg） | 97.9 | **0.00%** |
| 32（sendmmsg） | 95.2 | 4.78% |
| 64（sendmmsg） | 94.2 | 5.76% |

burst=8 でシステムコール頻度が 1/8 に削減された結果、100 MSPS で 36% → 0% drops を達成した。

#### 全速送信でのスループット改善

| シナリオ | burst=1 (SG) | burst=64 (mmsg) | 改善率 |
|---|---|---|---|
| 1ch × 668 | 97.7 MSPS | 108.3 MSPS | **+11%** |
| 1ch × 256 | 31.8 MSPS | 42.5 MSPS | **+34%** |

### 4.3 最適バーストサイズ

| burst_size | 特性 | 推奨用途 |
|---|---|---|
| 1 | Phase 9.1 互換。最低レイテンシ | デフォルト、低レート |
| 8 | rate-limited シナリオで最もバランスが良い | 100 MSPS ターゲット |
| 64 | unlimited throughput で最大値を記録 | 最大スループット要求時 |
| 32 | ノイズの影響を受けやすく一貫性に欠ける | 非推奨 |

WSL2 の仮想ネットワークスタックは測定ごとに 10–20% の変動がある。burst=32 で異常なドロップ率（46%）が観測されるケースがあり、大きいバーストサイズでは WSL2 固有の不安定性が顕在化する。

### 4.4 Windows native 回帰検証

scatter-gather WSASendTo への変更が Windows native 性能に回帰を与えないことを確認した。

| datagram | Phase 9.1 (memcpy+sendto) | Phase 9.2 (WSASendTo) | Delta |
|---|---|---|---|
| 1,400 B (1ch) | ~147 MSPS, 0% drops | 133.9 MSPS, 0% drops | −9%（ノイズ） |
| 65,000 B (1ch) | 3,371 MSPS, 0% drops | 3,321 MSPS, 0% drops | **−1.5%**（ノイズ範囲） |
| 65,000 B (4ch) | — | 773 MSPS, 0% drops | — |
| 1 GSPS target | 1,000 MSPS, 0% drops | 1,000 MSPS, 0% drops | **同等** |

全シナリオで 0% drops を維持し、スループットはノイズ範囲内の差異に留まった。

---

## 5. 環境とツールチェーン

### ハードウェア

- **CPU**: AMD Ryzen 9 9950X3D (16C/32T, L3 64 MB)
- **GPU**: NVIDIA GeForce RTX 5080
- **NIC**: ループバック（localhost）のみ使用

### ソフトウェア

| 項目 | WSL2 (開発・測定) | Windows native (測定) |
|------|------|------|
| OS | Ubuntu 24.04 (WSL2 6.6.87.2) | Windows 11 |
| コンパイラ | GCC (C++20, -O2) | MSVC 19.44 |
| ビルドシステム | CMake Release preset | CMake + Ninja |
| ベンチマークツール | `grebe-bench --udp --duration=5` | 同左 |

### 測定条件

- 全測定は 5 秒間のループバック送受信
- WSL2 はデータグラムサイズ 1,400 bytes（MTU 制限）
- Windows native は 1,400 / 4,000 / 16,000 / 32,000 / 65,000 bytes で測定
- Phase 9.2 では `--udp-burst=N` オプションで burst_size を指定

---

## 6. 課題と今後の展望

### 6.1 検証済みの範囲

- WSL2 での UDP スループット上限と特性（~104 MSPS, 155K fps）
- Windows native での大データグラムスケーリング（~3,400 MSPS）
- 1ch × 1 GSPS via UDP の達成（Windows native, ≥16KB datagram）
- scatter-gather I/O の実装と回帰なしの確認
- sendmmsg/recvmmsg による WSL2 100 MSPS 0% drops 達成
- 4ch マルチチャネルの合計スループット特性

### 6.2 未検証・残課題

| 課題 | 説明 | 優先度 |
|------|------|--------|
| Linux native 測定 | WSL2 は MTU 制限あり。Linux native での 65KB datagram + sendmmsg 性能が未測定 | 中 |
| Pipe vs UDP E2E 比較 | viewer FPS を含む E2E 性能の Pipe/UDP 比較（T-2）が未実施 | 中 |
| UDP E2E レイテンシ | UDP モードでの各レート E2E レイテンシ（T-4）が未実施 | 中 |
| ネットワーク越し測定 | ループバックのみ。実ネットワーク（1GbE/10GbE/25GbE）での性能は未検証 | 低 |
| IOCP backend | Windows の非同期 I/O。既に 3,400 MSPS で十分なため見送り | 低 |
| RIO backend | Registered I/O。Win8+ 限定、PoC では過剰 | 低 |

### 6.3 設計上の判断

| 施策 | 判断 | 理由 |
|------|------|------|
| 大データグラム（≥16KB） | Windows native で有効 | 1 GSPS 達成に必須。WSL2 は MTU 制限で不可 |
| sendmmsg バッチ化 | Linux で有効、burst=8 推奨 | 100 MSPS 0% drops 達成。Windows は API なし |
| scatter-gather I/O | 全環境で採用 | 回帰なし確認済み。memcpy 廃止はアーキテクチャ上正しい |
| 共有メモリトランスポート | 別 Phase | 4ch × 1 GSPS (8 GB/s) は UDP では不可能。RDD v3.1 の SharedMemory で対応 |

---

## 7. 結論

本調査は、UDP トランスポートの性能特性について以下の知見を得た。

1. **UDP ループバックのボトルネックはシステムコールオーバーヘッドである**。フレームレート上限は WSL2 で ~155K fps、Windows native で ~220K fps であり、ペイロードサイズに依存しない。1 フレームあたりの syscall コストは WSL2 で 6.4 μs、Windows native で 4.5 μs であり、この差は Hyper-V 仮想化のオーバーヘッドに起因する。

2. **データグラムサイズの拡大が最も効果的な最適化である**。フレームレートが一定である以上、per-frame のサンプル数を増やすことが MSPS 向上の最短経路である。1,400 → 65,000 bytes の拡大で MSPS は 22.2 倍に向上し、1 GSPS via UDP を 0% drops で達成した。ただしこの最適化は Windows native に限定され、WSL2 は MTU 制限により適用できない。

3. **sendmmsg/recvmmsg は中帯域（~100 MSPS）で最も効果的である**。全速送信でのスループット改善は +11% に留まるが、100 MSPS ターゲットでのドロップ率は 36% → 0% と劇的に改善した。バッチ化はシステムコール頻度を 1/N に削減するため、帯域がシステムコール上限に近い領域で最大の効果を発揮する。

4. **合計スループットはチャネル数に依存しない**。1ch と 4ch で合計 MSPS は同等（~3,400 MSPS @ 65KB）であり、ボトルネックはチャネル処理ではなくデータグラム転送にある。マルチチャネルでは合計帯域が等分されるため、4ch × 1 GSPS/ch（8 GB/s）はシステムコールベースの UDP では非現実的であり、共有メモリデータプレーンが必要となる。

これらの知見は、RDD v3.1 における SharedMemory データプレーンの設計根拠を補強する。UDP は 100 MSPS 以下の ingress やネットワーク越しの運用で十分な性能を持つが、同一マシン上の高帯域マルチチャネル通信には SharedMemory が必要であるという v3.1 の方針は、本調査の定量的エビデンスに裏付けられている。
