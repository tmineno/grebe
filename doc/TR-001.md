# TR-001: 高速リアルタイム波形描画の実装技術と評価

**Document ID**: TR-001
**Project**: grebe — Vulkan Stream PoC
**Date**: 2026-02-11
**Status**: Final (PoC Phase 0–13.5 完了時点)

---

## 概要

本レポートは、Vulkan ベースの高速時系列データストリーム可視化 PoC「grebe」において、1 GSPS（10⁹ samples/sec）16-bit ADC データのリアルタイム波形描画を実現するために採用した実装技術、その定量的評価結果、および今後の課題と展望を報告する。

### 主要な成果

| 指標 | 目標 | 達成値 | 環境 |
|------|------|--------|------|
| L2 (1 GSPS, V-Sync ON) | 60 FPS | 59.7 FPS | RTX 5080, MSVC, Windows |
| L3 (1 GSPS, V-Sync OFF) | 最大 FPS | 2,022 FPS | 同上 |
| E2E レイテンシ (p99) | ≤50 ms | 18.1 ms (最悪) | IPC, 1ch × 1 MSPS |
| 波形忠実度 | 100% | 100% (Embedded) | MinMax, ±1 LSB |
| マルチチャネル (4ch × 1 GSPS) | 0 drops | 0 drops | Embedded |

---

## 1. システムアーキテクチャ

### 1.1 全体構成

```
grebe-sg (Signal Generator)          grebe (Viewer)
┌──────────────────────┐             ┌──────────────────────────────────────┐
│  DataGenerator       │   Pipe /    │  PipeConsumer                        │
│  (per-channel        │──Embedded──▶│  ↓                                   │
│   period tiling)     │   IPC       │  N × RingBuffer (lock-free SPSC)     │
│                      │             │  ↓                                   │
│  SG UI (ImGui)       │             │  DecimationThread (coordinator       │
│                      │             │   + 1–4 workers)                     │
└──────────────────────┘             │  ↓                                   │
                                     │  BufferManager (triple buffering)    │
                                     │  ↓                                   │
                                     │  Renderer (Vulkan LINE_STRIP)        │
                                     │  ↓                                   │
                                     │  Viewer UI (ImGui overlay)           │
                                     └──────────────────────────────────────┘
```

データは生成スレッドからチャネル別リングバッファを経由し、デシメーションスレッドで画面解像度に縮約された後、GPU にアップロードされて描画される。各段階は独立スレッドで動作し、ロックフリーまたは最小限の同期で結合される。

### 1.2 設計原則：データ量削減の階層化

本システムの設計上最も重要な判断は、**GPU に送る前に CPU 側でデータ量を劇的に削減する**ことである。

| 段階 | データ量 | 削減率 |
|------|----------|--------|
| 入力 (1 GSPS × 2 bytes) | 2 GB/s | — |
| 表示ウィンドウ分のサンプル | 数十 MB/frame | — |
| MinMax デシメーション後 | 7.68 KB/frame | **約 4,000× 削減** |

1920 ピクセル幅のディスプレイに対し、MinMax アルゴリズムは 1920 × 2 = 3,840 頂点（min/max ペア）を出力する。入力サンプルレートに関わらず GPU 転送量は一定の 7.68 KB に収まるため、PCIe 帯域はボトルネックにならない。

---

## 2. 実装技術の詳細

### 2.1 CPU デシメーション：SSE2 SIMD MinMax

MinMax アルゴリズムは入力を等分割バケットに分け、各バケットの最小値・最大値のペアを出力する。波形のエンベロープ（包絡線）を正確に保存するため、振幅の極値が失われない。

#### SIMD 実装の構造

```
入力: int16_t[] (数百万〜数十億サンプル)
      ↓
┌─ バケット分割 (target_points / 2 バケット) ─────────────┐
│                                                          │
│  ┌─ 2× Unrolled SSE2 ループ (16 samples/iter) ─┐       │
│  │  __m128i a = _mm_loadu_si128(ptr);            │       │
│  │  __m128i b = _mm_loadu_si128(ptr+8);          │       │
│  │  vmin = _mm_min_epi16(vmin, a);               │       │
│  │  vmin = _mm_min_epi16(vmin, b);               │       │
│  │  vmax = _mm_max_epi16(vmax, a);               │       │
│  │  vmax = _mm_max_epi16(vmax, b);               │       │
│  └───────────────────────────────────────────────┘       │
│  ↓                                                       │
│  水平リダクション (hmin/hmax: shuffle + extract)          │
│  ↓                                                       │
│  出力: [bucket_min, bucket_max]                          │
└──────────────────────────────────────────────────────────┘
出力: int16_t[3840] (1920 × min/max ペア)
```

- **2× アンロール**: 1 イテレーションで 16 サンプル（128-bit レジスタ × 2）を処理し、ループオーバーヘッドを半減
- **水平リダクション**: `_MM_SHUFFLE` によるカスケーディングシャッフルで 8 要素の min/max を単一スカラに集約
- **テールハンドリング**: 8 要素チャンク → スカラで端数処理（アライメント非依存）

#### コンパイラ依存性の発見

| コンパイラ | MinMax Scalar | MinMax SIMD | SIMD 効果 |
|------------|---------------|-------------|-----------|
| GCC -O2 | 19,988 MSPS | 21,521 MSPS | 1.1× |
| MSVC /O2 | 1,884 MSPS | 19,834 MSPS | **10.5×** |

GCC は `-O2` で自動ベクタライズを行うためスカラコードが既に SIMD 相当の速度を出すが、MSVC は自動ベクタライズしないため明示的 SIMD intrinsics が不可欠である。クロスプラットフォーム開発では、コンパイラの自動最適化に依存せず明示的な SIMD コードを記述することが重要という知見を得た。

### 2.2 LTTB アルゴリズムとその限界

LTTB (Largest Triangle Three Buckets) は視覚的忠実度に優れたダウンサンプリングアルゴリズムであり、「前の選択点・現在の候補点・次バケットの平均」が形成する三角形面積を最大化する点を各バケットから選出する。

しかし計測の結果、LTTB のスループットは 734 MSPS（Release）に留まり、1 GSPS の入力に追従できないことが判明した。これに対し MinMax SIMD は約 20 GSPS と 27 倍の性能差がある。

**対策**: サンプルレート ≥ 100 MSPS で LTTB を自動的に MinMax に切り替えるガード機構を実装した。ユーザが選択したモード (`mode_`) とは別に実効モード (`effective_mode_`) を atomic 変数で管理し、高レート時は透過的にフォールバックする。

### 2.3 ロックフリー SPSC リングバッファ

データ生成スレッドとデシメーションスレッド間の結合には、ロックフリー Single-Producer Single-Consumer (SPSC) リングバッファを採用した。

#### 設計要点

- **アトミック変数のみ**: `head_`（producer 管理）と `tail_`（consumer 管理）の 2 つの `std::atomic<size_t>` で同期
- **メモリオーダリング**: 自スレッドの読み取りは `relaxed`、他スレッドのポインタ読み取りは `acquire`、ポインタ更新は `release`
- **バルク操作**: `push_bulk()` / `pop_bulk()` は最大 2 回の `memcpy`（ラップアラウンド境界での分割）で大量データを一括転送
- **ゼロアロケーション**: 初期化後のメモリ割り当てなし

1 GSPS 動作時でもリングバッファの充填率は 0.3% 未満に収まり、オーバーフローは発生しない。

### 2.4 マルチスレッドデシメーション

4ch / 8ch のマルチチャネル構成で 1 GSPS を達成するため、デシメーション処理を並列化した。

#### アーキテクチャ

```
Coordinator Thread
  │
  ├── work_cv_.notify_all()  (新作業を通知)
  │
  ├── Worker 0: drain ch0, ch4 → decimate
  ├── Worker 1: drain ch1, ch5 → decimate
  ├── Worker 2: drain ch2, ch6 → decimate
  └── Worker 3: drain ch3, ch7 → decimate
  │
  ├── done_cv_.wait()  (全 worker 完了待ち)
  │
  └── 結果結合 → front/back バッファ swap
```

- **チャネルラウンドロビン**: `ch % num_workers` で静的割当、ワーカー間のデータ依存なし
- **条件変数同期**: `std::barrier` のスピンウェイト問題（Windows/MSVC で CPU 消費過大）を回避するため、`std::condition_variable` + 作業世代カウンタを採用
- **キャッシュ局所性**: 各ワーカーが 1–2 チャネルのみを担当し、L3 キャッシュヒット率を改善

#### 効果

| 構成 | デシメーション時間 | ドロップ |
|------|---------------------|----------|
| 4ch × 1G（シングルスレッド） | 18.19 ms | 2.13G drops |
| 4ch × 1G（4 ワーカー） | 0.22–0.46 ms | **0 drops** |

40–80 倍の高速化により、マルチチャネル高レート時のドロップを完全に解消した。

### 2.5 GPU トリプルバッファリング

BufferManager は 3 つのバッファスロットで CPU アップロード・DMA 転送・GPU 描画を重畳する。

```
Slot 0: [CPU write]    ←── decimation thread が staging に memcpy
Slot 1: [DMA transfer]  ←── vkCmdCopyBuffer (staging → device)
Slot 2: [GPU draw]      ←── renderer が device buffer を参照
```

- **VMA (Vulkan Memory Allocator)** による自動メモリタイプ選択
- **ステージングバッファ**: `HOST_ACCESS_SEQUENTIAL_WRITE` + `MAPPED`（永続マップ）
- **デバイスバッファ**: `AUTO_PREFER_DEVICE`（GPU ローカル）
- **フェンス同期**: スロット再利用前に `vkWaitForFences` で転送完了を保証
- **動的拡張**: データ量がスロット容量を超えた場合に自動リアロケーション

### 2.6 Vulkan レンダリングパイプライン

#### 頂点フォーマットとプッシュ定数

- **頂点**: `VK_FORMAT_R16_SINT`、ストライド 2 bytes — 16-bit 符号付き整数をそのまま GPU に送信し、シェーダ内でスケーリング
- **プッシュ定数** (48 bytes/draw call):

```c
struct WaveformPushConstants {
    float amplitude_scale;     // 振幅スケール
    float vertical_offset;     // 垂直オフセット
    float horizontal_scale;    // 時間軸スケール
    float horizontal_offset;   // 時間軸オフセット
    int   vertex_count;        // チャネルの頂点数
    int   first_vertex;        // 共有バッファ内のオフセット
    float color_r, g, b, a;    // チャネル固有色
};
```

#### マルチチャネル描画

全チャネルの頂点データを単一の頂点バッファに連結し、チャネルごとにプッシュ定数を切り替えて `vkCmdDraw` を発行する。バッファバインドは 1 回で済み、ドローコールのオーバーヘッドのみがチャネル数に比例する。

### 2.7 データ生成の最適化：周期タイリング

≥ 100 MSPS の高レートでは、1 周期分の波形を事前計算したバッファ（period buffer）を `memcpy` で繰り返しコピーすることで、サンプルあたりの三角関数演算を排除する。

```
period_buf: [1 周期分の sin 波形 (e.g., 4096 samples)]
            ↓ memcpy × N 回
ring_buffer: [...period...period...period...]
```

低レート（< 100 MSPS）やチャープ波形では LUT（4096 エントリ四分円テーブル）と位相累積によるサンプル単位生成にフォールバックする。

### 2.8 IPC トランスポート

#### プロセス分離アーキテクチャ

Phase 8 で可視化プロセス (grebe) と信号生成プロセス (grebe-sg) を分離し、匿名パイプで接続する 2 プロセスモデルを実装した。

#### フレームヘッダプロトコル (FrameHeaderV2)

```c
struct FrameHeaderV2 {
    uint32_t magic;                // 'GFH2'
    uint64_t sequence;             // 単調増加、ギャップ検出用
    uint64_t producer_ts_ns;       // E2E レイテンシ計測用
    uint32_t channel_count;        // 1–8
    uint32_t block_length_samples; // 1024–65536
    uint32_t payload_bytes;        // channel_count × block_length × 2
    double   sample_rate_hz;       // SG → grebe レート同期
    uint64_t sg_drops_total;       // SG 側ドロップ累計
    uint64_t first_sample_index;   // 絶対サンプル位置
};
```

#### パイプ最適化

| 設定 | 最適化前 | 最適化後 | 改善率 |
|------|----------|----------|--------|
| パイプバッファ | OS デフォルト | 1 MB | — |
| ブロックサイズ | 1024 | 16384 | — |
| スループット (Windows) | ~10 MB/s | 100–470 MB/s | 10–47× |

---

## 3. ボトルネック分析と知見

### 3.1 GPU コンピュートシェーダ vs CPU デシメーション

GPU コンピュートシェーダによる MinMax デシメーションを実装・計測した結果、CPU SIMD の方が 3.9–7.1 倍高速であった。

| 手法 | スループット | 備考 |
|------|-------------|------|
| CPU MinMax SIMD (MSVC) | 19,834 MSPS | SSE2, 2× unroll |
| GPU Compute (RTX 5080) | 5,127 MSPS | dispatch + readback 含む |

**根本原因**: デシメーション後のデータ量が 7.68 KB と極めて小さいため、GPU コンピュートのディスパッチオーバーヘッドとリードバックレイテンシが支配的となる。大量データを GPU に送って GPU 上で削減するよりも、CPU 側で削減してから少量データを GPU に送る方が効率的である。

### 3.2 Viewer 側の真のボトルネック：ドレイン処理とキャッシュコールドデータ

Phase 10 のボトルネック分析で、マルチチャネル高レート時のドロップ原因はパイプ転送速度ではなく、**消費側のパイプラインドレイン処理とキャッシュコールドデータアクセス**であることが判明した。

**根拠**: パイプを完全に排除した Embedded モード（プロセス内直結）でも同等のドロップが発生した。

| 条件 | パイプライン実効スループット | BM-B 理論値 | 利用率 |
|------|------|------|------|
| 4ch × 1G (Embedded) | 3.75 GSPS | 21.5 GSPS | 17% |

BM-B（孤立ベンチマーク）では 21.5 GSPS を達成するデシメーション処理が、実パイプラインでは 3.75 GSPS（17%）しか発揮できない。この 83% もの性能低下の原因は以下の 2 つである。

#### 3.2.1 パイプラインドレイン処理のオーバーヘッド

リングバッファは循環構造であるため、消費側（デシメーションスレッド）がデータを読み出す際には `pop_bulk()` による**線形バッファへのコピー（ドレイン）**が必要となる。

```
リングバッファ (循環構造):
  [...データ後半...][...データ前半...]
       tail ──────────▶ head
                ↓ pop_bulk() (最大 2 回の memcpy)
  線形バッファ (history_buf):
  [...データ前半...][...データ後半...]
```

このドレインは単純な `memcpy` だが、4ch × 1 GSPS では 1 フレームあたり約 34 MB/ch × 4ch = 136 MB のデータコピーが発生する。さらに、ドレイン後のデータはヒストリバッファに追記され、表示ウィンドウに収まるようトリミングされた後にデシメーション関数に渡される。この一連の処理が BM-B（事前に線形配列を用意した孤立ベンチマーク）には存在しないオーバーヘッドである。

#### 3.2.2 キャッシュコールドデータアクセス

BM-B と実パイプラインの性能乖離の主因は **CPU キャッシュの振る舞いの違い**にある。

- **BM-B（孤立ベンチマーク）**: 同一のデータ配列を繰り返しデシメーションする。2 回目以降のイテレーションでは対象データが L2/L3 キャッシュに載っており、メモリアクセスはほぼキャッシュヒットとなる。
- **実パイプライン**: リングバッファから毎フレーム新規データをドレインする。このデータは DataGenerator スレッドが書き込んだものであり、デシメーションスレッドの CPU コアから見ると**一度もアクセスしていないキャッシュコールドなメモリ領域**である。

4ch × 1 GSPS における 1 フレームの処理データ量 136 MB は、Ryzen 9 9950X3D の L3 キャッシュ容量（64 MB）を 2 倍以上超過する。シングルスレッドで 4 チャネルを逐次処理する場合、ch0 のデータが L3 に載った頃には ch3 の処理でそれが追い出され、フレームごとにキャッシュの再充填が発生する。

```
シングルスレッド（Phase 10-2 以前）:
  ch0 (34MB) → ch1 (34MB) → ch2 (34MB) → ch3 (34MB)
  ──────────────────────────────────────────────────▶ 時間
  L3 (64MB): [ch0][ch1] → ch2 が ch0 を追い出す → キャッシュミス連発

マルチスレッド（Phase 10-3）:
  Worker 0: ch0 (34MB)   Worker 1: ch1 (34MB)
  Worker 2: ch2 (34MB)   Worker 3: ch3 (34MB)
  ──────────────────▶ 時間 (並列実行)
  各 Worker: 34MB ≪ L3 64MB → キャッシュ内に収まる
```

#### 3.2.3 解決策と効果

この分析により、共有メモリ IPC の実装は不要と判断した（トランスポートはボトルネックではないため）。代わりにマルチスレッドデシメーション（Phase 10-3）を導入し、チャネルごとのキャッシュ局所性を改善することで解決した。各ワーカーが 1–2 チャネルのみを担当するため、ワーキングセットが L3 キャッシュに収まり、デシメーション時間は 18.19 ms → 0.22–0.46 ms（40–80 倍）に短縮、ドロップは 0 になった。

### 3.3 信号生成側 (SG) のボトルネック

Viewer 側のドロップが解消された後も、IPC モードの高レートマルチチャネル構成では **SG 側でドロップが発生**する。これはパイプ帯域の物理的上限に起因する。

#### 3.3.1 ドロップ発生メカニズム

SG プロセスでは、DataGenerator スレッドと Sender スレッドの 2 スレッドがリングバッファを介して結合される。

```
DataGenerator thread          Sender thread
  │                              │
  │ push_bulk() で               │ pop_bulk() で
  │ ring に書き込み              │ ring から読み出し
  │         ↓                    │         ↓
  │  ┌─────────────┐            │  writev() で
  │  │ Ring Buffer  │───────────▶│  パイプに送信
  │  │ (SPSC)       │            │
  │  └─────────────┘            │
  │                              │
  ▼ ring full → push_bulk()     ▼ パイプ満杯 → writev()
    が部分書込み → ドロップ        がブロック → ring が溜まる
```

1. DataGenerator が目標レートでサンプルを生成し、`push_bulk()` でリングバッファに書き込む
2. Sender スレッドがリングバッファからデータを読み出し、`writev()` でパイプに送信
3. パイプバッファ（1 MB）が満杯になると `writev()` がブロックし、Sender が停滞
4. Sender が停滞するとリングバッファが充填 → DataGenerator の `push_bulk()` が部分書込みとなりドロップ発生
5. ドロップは `DropCounter` で計上され、`FrameHeaderV2.sg_drops_total` として Viewer に通知

#### 3.3.2 定量的なドロップ率

| 構成 | SG ドロップ | ドロップ率 | 必要帯域 | パイプ帯域 | 帯域ギャップ |
|------|-----------|----------|----------|-----------|-------------|
| 1ch × 1G IPC | 0 | 0% | 2 GB/s | ~410 MB/s | 4.9× |
| 4ch × 1G IPC | 7.9B | **37%** | 8 GB/s | ~410 MB/s | ~20× |
| 8ch × 1G IPC | 34.3B | **79%** | 16 GB/s | ~410 MB/s | ~39× |
| 全構成 ≤ 100 MSPS | 0 | 0% | ≤ 200 MB/s | ~410 MB/s | 十分 |

1ch × 1 GSPS ではパイプ帯域が辛うじて足りるが、4ch 以上では要求帯域がパイプの物理限界を大幅に超過する。

#### 3.3.3 フロー制御の不在

現在の実装では、高レート（≥ 100 MSPS）時のバックプレッシャー機構が意図的に無効化されている。

- **低レート（< 100 MSPS）**: リングバッファ充填率 > 90% で 100 µs のスリープを挿入し、生成レートを抑制
- **高レート（≥ 100 MSPS）**: スリープ粒度がサブミリ秒の精度を保証できないため、ビジーウェイトペーシングのみ。DataGenerator は Sender の停滞を関知せず、目標レートでの生成を継続する

この設計選択は「1 GSPS 生成能力の検証」という PoC 目標に基づく。バックプレッシャーを有効にすると実効生成レートがパイプ帯域に制約され、DataGenerator 自体の性能を正確に評価できなくなるためである。

#### 3.3.4 波形描画品質への影響

SG ドロップは描画の空間解像度には影響しない。MinMax デシメーションは入力サンプル数に関わらず常に 3,840 頂点/ch を出力するためである。

| 影響項目 | SG ドロップなし | SG ドロップあり | 影響度 |
|----------|---------------|---------------|--------|
| 頂点数/ch | 3,840 | 3,840 | なし |
| 時間ウィンドウ密度 | 高 | 低（間引き） | 中 |
| 振幅ピーク保存 | 完全 | 完全 | なし |
| エンベロープ一致率 | 100% | 99.2% (4ch×1G) | 軽微 |

ドロップにより入力サンプルが疎になるため表示時間窓のカバレッジは低下するが、到着したサンプルに対する MinMax 処理は正確に行われ、波形のピーク/トラフは保存される。

#### 3.3.5 プロダクションに向けた緩和策

| 戦略 | 概要 | 効果 | 適用時期 |
|------|------|------|----------|
| SG 側プリデシメーション | SG で MinMax 後に転送（8 GB/s → ~8 MB/s） | ドロップ根絶 | プロダクション |
| クレジットベースフロー制御 | Viewer → SG への受信可能通知 | 過剰生成防止 | プロダクション |
| 共有メモリ IPC | パイプ → shm 置換（帯域制約解消） | 帯域 10–100× | 実デバイス I/O 時 |
| 適応レート制限 | リング充填率に応じて生成レート動的調整 | SG ドロップ ≈ 0 | プロダクション |

PoC では SG ドロップを許容する判断としたが、プロダクション移行時は **SG 側プリデシメーション**（転送量を約 1,000 分の 1 に削減）が最も費用対効果の高い解決策である。

### 3.4 描画プリミティブの余裕度

LINE_STRIP の描画性能はボトルネックから遠く離れている。

| 頂点数 | FPS (V-Sync OFF) | 備考 |
|--------|-------------------|------|
| 3,840 (標準) | 2,022 | MinMax 出力量 |
| 38,400 (10×) | 3,909 | — |
| 384,000 (100×) | 3,741 | — |

100 倍の頂点数でも描画性能はほぼ変わらず、RTX 5080 の描画能力に対して大きなマージンがある。

---

## 4. 波形忠実度の検証

### 4.1 エンベロープ検証器

波形の正確性を定量的に保証するため、EnvelopeVerifier を実装した。入力信号の理論的なエンベロープ（周期信号のスライディングウィンドウ min/max）と、パイプライン出力の MinMax デシメーション結果を ±1 LSB の許容誤差で比較する。

#### 検証方式

1. 既知の周期信号（正弦波等）の 1 周期バッファから、デシメーションバケットサイズに応じた理論エンベロープをオンデマンド生成・キャッシュ
2. 各フレームのデシメーション出力（min/max ペア）を理論値と比較
3. 一致率をパーセンテージで報告

#### 検証結果

| 構成 | モード | エンベロープ一致率 |
|------|--------|-------------------|
| Embedded 1ch, 全レート | MinMax | **100%** |
| Embedded 4ch, 全レート | MinMax | **100%** |
| IPC 1ch, 全レート | MinMax | **100%** |
| IPC 4ch, ≤ 100 MSPS | MinMax | **100%** |
| IPC 4ch, 1 GSPS | MinMax | 99.2% |

IPC 4ch × 1 GSPS での 0.8% の不一致は SG 側のドロップに起因する位相不連続であり、パイプライン自体の忠実度低下ではない。

---

## 5. E2E レイテンシ評価

FrameHeaderV2 の `producer_ts_ns`（データ生成時刻）から `vkQueuePresentKHR` 完了時刻までのエンドツーエンドレイテンシを計測した。

| シナリオ | 平均 | p99 | NFR 目標 | 判定 |
|----------|------|-----|----------|------|
| Embedded 1ch, 10–1000 MSPS | 1.5–1.6 ms | 1.9–2.3 ms | L1 ≤ 50 ms | **PASS** |
| Embedded 4ch, 1–1000 MSPS | 1.5–2.6 ms | 2.0–6.6 ms | L2 ≤ 100 ms | **PASS** |
| IPC 1ch, 10–1000 MSPS | 1.7–2.3 ms | 2.3–3.3 ms | L1 ≤ 50 ms | **PASS** |
| IPC 4ch, 10–1000 MSPS | 2.2–2.6 ms | 3.3–4.3 ms | L2 ≤ 100 ms | **PASS** |

最悪ケース（IPC 1ch × 1 MSPS）でも p99 = 18.1 ms であり、目標の 50 ms を大幅に下回る。レイテンシの支配項は Vulkan のレンダリングパイプライン（フェンス待ち + コマンド記録 + プレゼント）であり、データ転送やデシメーション処理ではない。

---

## 6. マイクロベンチマーク総括

### BM-A: CPU → GPU 転送帯域

| 環境 | 帯域 |
|------|------|
| llvmpipe (CPU) | 10.3 GB/s |
| Mesa dzn (RTX 5080, D3D12 翻訳) | 23.7 GB/s |
| NVIDIA Native (RTX 5080) | 19.7 GB/s |

デシメーション後のフレームあたり転送量 7.68 KB に対し、いずれの環境でも十分な帯域が確保されている。

### BM-B: デシメーションスループット

| アルゴリズム | GCC Release | MSVC Release |
|-------------|-------------|--------------|
| MinMax Scalar | 19,988 MSPS | 1,884 MSPS |
| MinMax SIMD | 21,521 MSPS | 19,834 MSPS |
| LTTB | 734 MSPS | 743 MSPS |

### BM-C: 描画 FPS (V-Sync OFF)

| 環境 | 3,840 vtx | 38,400 vtx |
|------|-----------|------------|
| Mesa dzn | 770 FPS | — |
| NVIDIA Native | **2,022 FPS** | 3,909 FPS |

### BM-E: GPU コンピュートデシメーション

| 環境 | スループット |
|------|-------------|
| NVIDIA Native | 5,127 MSPS |

---

## 7. 環境とツールチェーン

### ハードウェア

- **CPU**: AMD Ryzen 9 9950X3D (16C/32T, L3 64 MB)
- **GPU**: NVIDIA GeForce RTX 5080
- **RAM**: 十分な容量（具体値は PoC では非制約）

### ソフトウェア

| 項目 | WSL2 (開発) | Windows Native (計測) |
|------|-------------|----------------------|
| OS | Ubuntu (WSL2 6.6.x) | Windows |
| コンパイラ | GCC (C++20) | MSVC 19.44 |
| GPU ドライバ | Mesa dzn (D3D12→Vulkan) | NVIDIA Native Vulkan |
| ビルドシステム | CMake 3.24+ | CMake + Ninja |
| 依存関係 | FetchContent (全自動) | 同左 |

### 依存ライブラリ

GLFW, vk-bootstrap, VMA (Vulkan Memory Allocator), glm, spdlog, nlohmann/json, stb, Dear ImGui — すべて CMake FetchContent で取得。

---

## 8. 課題と今後の展望

### 8.1 PoC で検証済みの範囲

- 1 GSPS 16-bit データの 60 FPS リアルタイム描画（L2 達成）
- V-Sync OFF での最大描画性能（L3 = 2,022 FPS）
- マルチチャネル（最大 8ch）のゼロドロップ動作（Embedded モード）
- 波形忠実度 100%（エンベロープ検証）
- E2E レイテンシ p99 < 20 ms
- CPU デシメーション vs GPU コンピュートの性能比較
- IPC パイプのスループット限界と最適化
- マルチスレッドデシメーションによるキャッシュボトルネック解消

### 8.2 未検証・残課題

| 課題 | 説明 | 優先度 |
|------|------|--------|
| 実デバイス I/O | ADC/FPGA からの実データ入力。現在は合成データ生成のみ | 高 |
| SG 側プリデシメーション | IPC 帯域不足（4ch × 1G で 37% ドロップ）の根本解決策。SG 側でデシメーションし転送量を 8 GB/s → 8 MB/s に削減 | 中 |
| 共有メモリ IPC | パイプ帯域が実デバイス I/O で不足する場合のトランスポート代替 | 低（現時点） |
| トリガ機能 | 内部/外部トリガによる波形キャプチャ | 中 |
| CRC 検証 | FrameHeaderV2 の CRC32C によるデータ整合性保証 | 低 |
| Persistent Mapped Buffers | ReBAR/SAM 対応 GPU での転送最適化（dzn 未対応のため未検証） | 低 |
| プロダクションレベルの忠実度指標 | ピークミス率、極値振幅誤差など定量的品質メトリクス | 中 |

### 8.3 プロダクション移行に向けた推奨事項

1. **実デバイス統合を最優先**: PoC の全パフォーマンス目標は達成済み。次のリスクは実 ADC/FPGA データの特性（バースト性、ジッタ、欠損）への対応にある。

2. **SG 側プリデシメーションの導入**: IPC モードでの高レートマルチチャネル（4ch × 1 GSPS 以上）は SG 側でデシメーション後に転送することで、パイプ帯域の制約を根本的に解消できる。

3. **コンパイラ依存性への対策**: MSVC 環境では明示的 SIMD intrinsics が必須。AVX2/AVX-512 への拡張で更なる高速化の余地がある（現在は SSE2 のみ）。

4. **キャッシュ効率の継続的最適化**: 8ch × 1 GSPS でのキャッシュコールドデータ問題は解決済みだが、チャネル数やサンプルレートの更なる増加に備え、プリフェッチやメモリアクセスパターンの最適化を継続すべきである。

---

## 9. 結論

本 PoC は、以下の技術的仮説を定量的に検証した。

1. **CPU 側 MinMax デシメーション + Vulkan 描画の組み合わせで 1 GSPS リアルタイム可視化は実現可能である**。SSE2 SIMD による MinMax は約 20 GSPS のスループットを持ち、1 GSPS 入力に対して 20 倍のマージンがある。

2. **データ量削減が最も効果的な最適化である**。GPU コンピュートシェーダによるデシメーションよりも、CPU 側でデシメーションして転送量を 7.68 KB/frame に抑える方が 3.9–7.1 倍高速である。

3. **ボトルネックは直感に反する場所にある**。IPC トランスポートではなくキャッシュコールドデータアクセスが真の制約であり、計測駆動の分析なしには特定できなかった。

4. **波形忠実度と性能は両立する**。エンベロープ検証 100%（Embedded 全レート）を達成しつつ、2,022 FPS の描画性能を実現した。

計測駆動開発（Measurement-Driven Development）のアプローチにより、各フェーズで定量的な根拠に基づいた技術判断を積み重ね、限られた PoC 期間内で目標性能を達成できた。
